# -*- coding: utf-8 -*-
"""Shallow_Neural_Network_in_Pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SE8DY1Id3NpMBpKF2DlwIy3r36oErS-y

# Load Dependencies
"""

import torch
import torch.nn as nn

from torchvision.datasets import MNIST
from torchvision import transforms

from torchsummary import summary
import matplotlib.pyplot as plt

from torch.utils.data import DataLoader

"""# Load Data"""

train = MNIST("data", train = True, transform=transforms.ToTensor(), download=True)
test = MNIST("data",train=False, transform=transforms.ToTensor())

"""Totensor() scales pixles from [0,255] to [0,1]"""

train.data.shape

train.data[0]

plt.imshow(train.data[0].numpy().squeeze(),cmap="gray_r")

train.targets[0:100]

train.targets.shape

test.data.shape

test.targets.shape

"""# Batch Data"""

train_loader = torch.utils.data.DataLoader(train,batch_size=128,shuffle=True)
test_loader = torch.utils.data.DataLoader(test,batch_size=128)

"""DataLoader() can also sample and run multithread over a set number of workers"""

x_sample, y_sample = next(iter(train_loader))

x_sample.shape

y_sample.shape

y_sample

x_sample[0]

x_flat_sample=x_sample.view(x_sample.shape[0],-1)

"""view() reshapes the tensor"""

x_flat_sample.shape

x_flat_sample[0]

"""# Design Neural Network Architecture"""

n_input = 784
n_dense = 64
n_out = 10

model = nn.Sequential(
    nn.Linear(n_input,n_dense), # hidden layer
    nn.Sigmoid(), # activation function
    nn.Linear(n_dense,n_out) # output layer
)

summary(model,(1,n_input))

"""# Configure training hyperparameters"""

cost_fxn = nn.crossEntropyLoss() # include softmax activation

optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

"""# Train"""

def accuracy_pct(pred_y, true_y):
  _,prediction = torch.max(pred_y,1) # returns maximum values, indices; fed tensor, dim to
  correct = (prediction == true_y).sum().item()
  return(correct/true_y.shape[0])*100.0

n_batches = len(train_loader)
n_batches

n_epochs = 20
print("Training for {} epochs. \n".format(n_epochs))
for epoch in range(n_epochs):
  avg_cost = 0.0
  avg_accuracy = 0.0

  for i, (x,y) in enumerate(train_loader): # enumerate() provides the count of iterations

    # forward propogation:

    # backprop and optimization via gradient descent:

    # Calculate accuracy metric:
    accuracy = accuracy_pct(y_hat,y)
    avg_accuracy += accuracy / n_batches

    if(i+1) % 100 == 0:
      print("Step {}".format(i+1))